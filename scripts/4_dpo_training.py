"""
DPO (Direct Preference Optimization) è®­ç»ƒè„šæœ¬ - Step 4
ä½¿ç”¨åå¥½å¯¹æ•°æ®è¿›è¡ŒDPOå¾®è°ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
import torch.nn.functional as F

from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments,
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt


class DPOTrainer:
    def __init__(self, base_dir: str = "./"):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé˜²æ­¢å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´æ‰¾ä¸åˆ°æ–‡ä»¶
        self.base_dir = Path(base_dir).resolve()
        self.data_dir = self.base_dir / "data"
        self.sft_model_dir = self.base_dir / "models" / "bert_sft"
        self.model_dir = self.base_dir / "models" / "bert_dpo"
        self.results_dir = self.base_dir / "results" / "dpo"

        # åˆ›å»ºç›®å½•
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {self.base_dir}")

        # DPOå‚æ•°
        self.beta = 0.1  # DPOæ¸©åº¦å‚æ•°ï¼ˆå½“å‰ç®€åŒ–ç‰ˆé‡Œä¸»è¦ä¿ç•™ä»¥å¤‡æ‰©å±•ï¼‰
        self.max_length = 256
        self.num_labels = 2

        # å¼ºåˆ¶åªç”¨æœ¬åœ°ç¼“å­˜ï¼Œç¦æ­¢è”ç½‘
        os.environ["TRANSFORMERS_OFFLINE"] = "1"

    def load_dpo_data(self):
        """åŠ è½½DPOåå¥½å¯¹æ•°æ®"""
        print("\nğŸ“Š åŠ è½½DPOåå¥½å¯¹æ•°æ®...")

        dpo_file = self.data_dir / "dpo_pairs" / "dpo_train.json"

        if not dpo_file.exists():
            print(f"âŒ DPOæ•°æ®ä¸å­˜åœ¨: {dpo_file}")
            print("è¯·å…ˆè¿è¡Œ: python scripts/1_data_preparation.py")
            return None

        with open(dpo_file, 'r', encoding='utf-8') as f:
            dpo_data = json.load(f)

        print(f"DPOè®­ç»ƒå¯¹æ•°: {len(dpo_data)}")

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(0.9 * len(dpo_data))
        train_data = dpo_data[:split_idx]
        eval_data = dpo_data[split_idx:]

        print(f"è®­ç»ƒé›†: {len(train_data)} | éªŒè¯é›†: {len(eval_data)}")

        return train_data, eval_data

    def label_text_to_id(self, label_text: str) -> int:
        """å°†æ ‡ç­¾æ–‡æœ¬æ˜ å°„åˆ° 0/1ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡"""
        if label_text is None:
            return 0
        t = str(label_text).strip()
        if t in ["positive", "æ­£é¢"]:
            return 1
        if t in ["negative", "è´Ÿé¢"]:
            return 0

        print(f"âš ï¸ æ„å¤–æ ‡ç­¾å€¼: {t}ï¼Œé»˜è®¤æ˜ å°„ä¸º 0ï¼ˆè´Ÿé¢ï¼‰")
        return 0

    def compute_dpo_loss(self, model, ref_model, batch):
        """
        åŸå§‹ DPO æŸå¤±ï¼ˆå½“å‰ç®€åŒ–ç‰ˆè„šæœ¬é‡Œæ²¡æœ‰ç›´æ¥ç”¨åˆ°ï¼Œä¿ç•™ä½œä¸ºå‚è€ƒï¼‰
        """
        # è·å–è¾“å…¥
        prompt_input_ids = batch['prompt_input_ids'].to(self.device)
        prompt_attention_mask = batch['prompt_attention_mask'].to(self.device)

        chosen_input_ids = batch['chosen_input_ids'].to(self.device)
        chosen_attention_mask = batch['chosen_attention_mask'].to(self.device)
        chosen_labels = batch['chosen_labels'].to(self.device)

        rejected_input_ids = batch['rejected_input_ids'].to(self.device)
        rejected_attention_mask = batch['rejected_attention_mask'].to(self.device)
        rejected_labels = batch['rejected_labels'].to(self.device)

        # å½“å‰ç­–ç•¥æ¨¡å‹çš„è¾“å‡º
        chosen_outputs = model(
            input_ids=chosen_input_ids,
            attention_mask=chosen_attention_mask,
            labels=chosen_labels
        )
        rejected_outputs = model(
            input_ids=rejected_input_ids,
            attention_mask=rejected_attention_mask,
            labels=rejected_labels
        )

        # å‚è€ƒæ¨¡å‹çš„è¾“å‡ºï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        with torch.no_grad():
            ref_chosen_outputs = ref_model(
                input_ids=chosen_input_ids,
                attention_mask=chosen_attention_mask,
                labels=chosen_labels
            )
            ref_rejected_outputs = ref_model(
                input_ids=rejected_input_ids,
                attention_mask=rejected_attention_mask,
                labels=rejected_labels
            )

        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        chosen_logps = -chosen_outputs.loss
        rejected_logps = -rejected_outputs.loss
        ref_chosen_logps = -ref_chosen_outputs.loss
        ref_rejected_logps = -ref_rejected_outputs.loss

        # DPOæŸå¤±
        pi_logratios = chosen_logps - rejected_logps
        ref_logratios = ref_chosen_logps - ref_rejected_logps

        loss = -F.logsigmoid(self.beta * (pi_logratios - ref_logratios)).mean()

        return loss

    def simplified_dpo_train(self):
        """ç®€åŒ–ç‰ˆDPOè®­ç»ƒï¼ˆåŸºäºåˆ†ç±»ä»»åŠ¡çš„æ”¹è¿›ï¼‰"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹DPOè®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼‰")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        result = self.load_dpo_data()
        if result is None:
            return

        train_data, eval_data = result

        # åŠ è½½SFTæ¨¡å‹ä½œä¸ºèµ·ç‚¹
        print(f"\nğŸ“¦ åŠ è½½SFTæ¨¡å‹: {self.sft_model_dir}")

        if not self.sft_model_dir.exists():
            print("âš ï¸  SFTæ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨baselineæ¨¡å‹")
            # ä¼˜å…ˆç”¨ baseline çš„ final ç›®å½•
            fallback_dir = self.base_dir / "models" / "bert_baseline" / "final"
            if fallback_dir.exists():
                self.sft_model_dir = fallback_dir
            else:
                # å†é€€ä¸€æ­¥ç”¨ bert-base-chinese é¢„è®­ç»ƒæ¨¡å‹
                self.sft_model_dir = self.base_dir / "models" / "bert-base-chinese"

        tokenizer = BertTokenizer.from_pretrained(
            str(self.sft_model_dir),
            local_files_only=True,
        )
        model = BertForSequenceClassification.from_pretrained(
            str(self.sft_model_dir),
            num_labels=self.num_labels,
            local_files_only=True,
        )

        # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šä½¿ç”¨ prompt ä½œä¸ºè¾“å…¥ï¼Œchosen ä½œä¸ºâ€œåå¥½æ ‡ç­¾â€
        print("\nğŸ”§ å‡†å¤‡DPOè®­ç»ƒæ•°æ®...")

        train_texts = []
        train_labels = []

        for item in train_data:
            # ä½¿ç”¨ prompt ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥åŠ ä¸Š â€œç­”æ¡ˆï¼šâ€ ä¿æŒå’Œ SFT ä¸€è‡´
            text = item["prompt"] + "\nç­”æ¡ˆï¼š"
            train_texts.append(text)

            # chosen æ˜¯äººå·¥è®¤ä¸ºæ­£ç¡®çš„æƒ…æ„Ÿæ ‡ç­¾ï¼Œä¾‹å¦‚ â€œæ­£é¢â€ / â€œè´Ÿé¢â€
            label_id = self.label_text_to_id(item.get("chosen"))
            train_labels.append(label_id)

        # Tokenize
        train_encodings = tokenizer(
            train_texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        train_dataset = Dataset.from_dict({
            'input_ids': train_encodings['input_ids'],
            'attention_mask': train_encodings['attention_mask'],
            'labels': torch.tensor(train_labels)
        })

        # éªŒè¯é›†ï¼šåŒæ ·ç”¨ prompt + chosen
        eval_texts = []
        eval_labels = []

        for item in eval_data:
            text = item["prompt"] + "\nç­”æ¡ˆï¼š"
            eval_texts.append(text)
            label_id = self.label_text_to_id(item.get("chosen"))
            eval_labels.append(label_id)

        eval_encodings = tokenizer(
            eval_texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        eval_dataset = Dataset.from_dict({
            'input_ids': eval_encodings['input_ids'],
            'attention_mask': eval_encodings['attention_mask'],
            'labels': torch.tensor(eval_labels)
        })

        # è®­ç»ƒå‚æ•°ï¼šæ•°æ®å¾ˆå°‘ï¼Œç”¨è¾ƒå°å­¦ä¹ ç‡ + è¾ƒå°‘ epochï¼Œé¿å…æŠŠ SFT è®­ç»ƒâ€œå¸¦åâ€
        training_args = TrainingArguments(
            output_dir=str(self.model_dir),
            num_train_epochs=2,              # epoch å°‘ä¸€ç‚¹
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            learning_rate=5e-6,             # æ¯” SFT æ›´å°çš„å­¦ä¹ ç‡
            weight_decay=0.01,
            warmup_ratio=0.1,
            logging_dir=str(self.results_dir / "logs"),
            logging_steps=20,
            eval_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=50,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            report_to="tensorboard",
            fp16=torch.cuda.is_available(),
        )

        def compute_metrics(eval_pred):
            predictions, labels = eval_pred
            predictions = np.argmax(predictions, axis=1)

            acc = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='weighted')
            f1_macro = f1_score(labels, predictions, average='macro')

            return {
                'accuracy': acc,
                'f1': f1,
                'f1_macro': f1_macro
            }

        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
        )

        # å¼€å§‹è®­ç»ƒ
        print("\nğŸ‹ï¸ å¼€å§‹DPOå¾®è°ƒ...")
        train_result = trainer.train()

        # ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜DPOæ¨¡å‹åˆ°: {self.model_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(self.model_dir)

        # è¯„ä¼°ï¼ˆéªŒè¯é›†ï¼‰
        print("\nğŸ“Š è¯„ä¼°DPOæ¨¡å‹ï¼ˆéªŒè¯é›†ï¼‰...")
        eval_results = trainer.evaluate(eval_dataset)

        print("\néªŒè¯é›†ç»“æœ:")
        for key, value in eval_results.items():
            try:
                print(f"  {key}: {value:.4f}")
            except TypeError:
                print(f"  {key}: {value}")

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼šå¤ç”¨ SFT çš„æµ‹è¯•æ•°æ®ï¼ˆæŒ‡ä»¤æ ¼å¼ï¼‰
        test_file = self.data_dir / "processed" / "sft_test.json"
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)

        test_texts = [f"{item['instruction']}\n{item['input']}\nç­”æ¡ˆï¼š" for item in test_data]
        # SFT æµ‹è¯•æ•°æ®ä¸­ï¼Œæ ‡ç­¾åœ¨ output å­—æ®µï¼Œå€¼ä¸º â€œæ­£é¢â€ / â€œè´Ÿé¢â€
        test_labels = [self.label_text_to_id(item.get("output")) for item in test_data]

        test_encodings = tokenizer(
            test_texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        test_dataset = Dataset.from_dict({
            'input_ids': test_encodings['input_ids'],
            'attention_mask': test_encodings['attention_mask'],
            'labels': torch.tensor(test_labels)
        })

        print("\nğŸ“Š è¯„ä¼°DPOæ¨¡å‹ï¼ˆæµ‹è¯•é›†ï¼‰...")
        test_results = trainer.evaluate(test_dataset)
        print("\næµ‹è¯•é›†ç»“æœ:")
        for key, value in test_results.items():
            try:
                print(f"  {key}: {value:.4f}")
            except TypeError:
                print(f"  {key}: {value}")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        predictions = trainer.predict(test_dataset)
        pred_labels = np.argmax(predictions.predictions, axis=1)

        report = classification_report(
            test_labels,
            pred_labels,
            target_names=['è´Ÿé¢', 'æ­£é¢'],
            digits=4
        )
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(report)

        # ä¿å­˜ç»“æœ
        results = {
            'model_name': 'BERT + SFT + DPO',
            'dpo_train_samples': len(train_data),
            'dpo_eval_samples': len(eval_data),
            'test_samples': len(test_data),
            'eval_results': eval_results,
            'test_results': test_results,
            'classification_report': report,
            'training_time': train_result.metrics.get('train_runtime', None),
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(self.results_dir / "results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_dir / 'results.json'}")

        return trainer, results


if __name__ == "__main__":
    trainer = DPOTrainer()
    trainer.simplified_dpo_train()

    print("\n" + "=" * 60)
    print("âœ… DPOè®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ: python scripts/5_evaluation.py")
