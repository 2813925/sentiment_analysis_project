"""
ç»¼åˆè¯„ä¼°è„šæœ¬ - Step 5
å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½
"""

import os
import json
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, classification_report
)

os.environ["TRANSFORMERS_OFFLINE"] = "1"  # åªç”¨æœ¬åœ°æ¨¡å‹


class ModelEvaluator:
    def __init__(self, base_dir: str = "./"):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…å·¥ä½œç›®å½•å˜åŒ–
        self.base_dir = Path(base_dir).resolve()
        self.data_dir = self.base_dir / "data" / "processed"
        self.models_dir = self.base_dir / "models"
        self.results_dir = self.base_dir / "results" / "comparison"

        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")

        # è¦è¯„ä¼°çš„æ¨¡å‹è·¯å¾„ï¼ˆbaseline ç”¨æœ€ç»ˆçš„ final ç›®å½•ï¼‰
        self.models = {
            "BERT Baseline": self.models_dir / "bert_baseline" / "final",
            "BERT + SFT": self.models_dir / "bert_sft",
            "BERT + SFT + DPO": self.models_dir / "bert_dpo",
        }

        self.max_length = 256

    def label_text_to_id(self, label_text: str) -> int:
        """å°†æ ‡ç­¾æ–‡æœ¬æ˜ å°„åˆ° 0/1ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡"""
        if label_text is None:
            return 0
        t = str(label_text).strip()
        if t in ["positive", "æ­£é¢"]:
            return 1
        if t in ["negative", "è´Ÿé¢"]:
            return 0
        print(f"âš ï¸ æ„å¤–æ ‡ç­¾å€¼: {t}ï¼Œé»˜è®¤æ˜ å°„ä¸º 0ï¼ˆè´Ÿé¢ï¼‰")
        return 0

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")

        with open(self.data_dir / "sft_test.json", "r", encoding="utf-8") as f:
            test_data = json.load(f)

        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")

        return test_data

    def evaluate_model(self, model_name, model_path, test_data):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ” è¯„ä¼°æ¨¡å‹: {model_name}")

        if not model_path.exists():
            print(f"  âš ï¸  æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            return None

        # åŠ è½½æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
        tokenizer = BertTokenizer.from_pretrained(
            str(model_path),
            local_files_only=True,
        )
        model = BertForSequenceClassification.from_pretrained(
            str(model_path),
            local_files_only=True,
        )
        model.to(self.device)
        model.eval()

        # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼šæŒ‡ä»¤ + è¾“å…¥ + â€œç­”æ¡ˆï¼šâ€
        texts = [f"{item['instruction']}\n{item['input']}\nç­”æ¡ˆï¼š" for item in test_data]

        # æ ‡ç­¾ï¼šä¼˜å…ˆç”¨ outputï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ï¼Œå…¼å®¹å¯èƒ½å­˜åœ¨çš„ sentiment å­—æ®µ
        true_labels = [
            self.label_text_to_id(
                item.get("output", item.get("sentiment"))
            )
            for item in test_data
        ]

        # æ‰¹é‡é¢„æµ‹
        predictions = []
        batch_size = 32

        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]

                inputs = tokenizer(
                    batch_texts,
                    padding="max_length",
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt",
                ).to(self.device)

                outputs = model(**inputs)
                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()
                predictions.extend(preds)

        predictions = np.array(predictions)
        true_labels = np.array(true_labels)

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, predictions)
        precision = precision_score(true_labels, predictions, average="weighted")
        recall = recall_score(true_labels, predictions, average="weighted")
        f1 = f1_score(true_labels, predictions, average="weighted")
        f1_macro = f1_score(true_labels, predictions, average="macro")

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, predictions)

        # åˆ†ç±»æŠ¥å‘Šï¼ˆè¿™é‡Œç”¨ä¸­æ–‡æ ‡ç­¾åï¼‰
        report = classification_report(
            true_labels,
            predictions,
            target_names=["è´Ÿé¢", "æ­£é¢"],
            digits=4,
        )

        results = {
            "model_name": model_name,
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "f1_macro": f1_macro,
            "confusion_matrix": cm.tolist(),
            "classification_report": report,
        }

        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
        print(f"  Macro F1: {f1_macro:.4f}")

        return results, predictions

    def qualitative_analysis(self, test_data, all_predictions):
        """å®šæ€§åˆ†æï¼šå±•ç¤ºå›°éš¾æ ·æœ¬"""
        print("\nğŸ”¬ å®šæ€§åˆ†æï¼šå›°éš¾æ ·æœ¬å¯¹æ¯”...")

        diff_samples = []

        for i, item in enumerate(test_data):
            true_label_id = self.label_text_to_id(
                item.get("output", item.get("sentiment"))
            )
            true_label = "positive" if true_label_id == 1 else "negative"

            preds = {name: preds[i] for name, preds in all_predictions.items()}

            # å¦‚æœé¢„æµ‹ä¸ä¸€è‡´ï¼Œæˆ–è€…ï¼ˆéƒ½ä¸€è‡´ä½†ï¼‰å’ŒçœŸå®æ ‡ç­¾ä¸ä¸€è‡´
            if len(set(preds.values())) > 1 or list(preds.values())[0] != true_label_id:
                # æ–‡æœ¬ï¼šä¼˜å…ˆç”¨ reviewï¼Œæ²¡æœ‰å°±ç”¨ input
                review_text = item.get("review", item.get("input", ""))

                diff_samples.append({
                    "index": i,
                    "review": review_text,
                    "true_label": true_label,
                    "predictions": {
                        name: "positive" if p == 1 else "negative"
                        for name, p in preds.items()
                    },
                })

        # ä¿å­˜åˆ†æç»“æœ
        output_file = self.results_dir / "qualitative_analysis.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(diff_samples[:20], f, ensure_ascii=False, indent=2)

        print(f"  å‘ç° {len(diff_samples)} ä¸ªå›°éš¾æ ·æœ¬")
        print(f"  ç¤ºä¾‹å·²ä¿å­˜åˆ°: {output_file}")

        # æ‰“å°å‡ ä¸ªç¤ºä¾‹
        print("\nç¤ºä¾‹åˆ†æï¼ˆå‰5ä¸ªï¼‰:")
        for i, sample in enumerate(diff_samples[:5]):
            print(f"\næ ·æœ¬ {i + 1}:")
            print(f"  è¯„è®º: {sample['review'][:60]}...")
            print(f"  çœŸå®æ ‡ç­¾: {sample['true_label']}")
            for model_name, pred in sample["predictions"].items():
                correct = "âœ“" if pred == sample["true_label"] else "âœ—"
                print(f"  {model_name}: {pred} {correct}")

    def plot_comparison(self, all_results):
        """ç»˜åˆ¶å¯¹æ¯”å›¾"""
        print("\nğŸ“Š ç»˜åˆ¶å¯¹æ¯”å›¾...")

        models = list(all_results.keys())
        metrics = {
            "Accuracy": [all_results[m]["accuracy"] for m in models],
            "F1 Score": [all_results[m]["f1"] for m in models],
            "Macro F1": [all_results[m]["f1_macro"] for m in models],
        }

        df = pd.DataFrame(metrics, index=models)

        # æŸ±çŠ¶å›¾
        fig, ax = plt.subplots(figsize=(12, 6))

        x = np.arange(len(models))
        width = 0.25

        bars1 = ax.bar(x - width, df["Accuracy"], width, label="Accuracy", alpha=0.8)
        bars2 = ax.bar(x, df["F1 Score"], width, label="F1 Score", alpha=0.8)
        bars3 = ax.bar(x + width, df["Macro F1"], width, label="Macro F1", alpha=0.8)

        ax.set_xlabel("Models", fontsize=12)
        ax.set_ylabel("Score", fontsize=12)
        ax.set_title("Model Performance Comparison", fontsize=14, fontweight="bold")
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=15, ha="right")
        ax.legend()
        ax.grid(axis="y", alpha=0.3)
        ax.set_ylim([0.7, 1.0])

        # æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2, bars3]:
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width() / 2.0,
                    height,
                    f"{height:.3f}",
                    ha="center",
                    va="bottom",
                    fontsize=9,
                )

        plt.tight_layout()
        plt.savefig(
            self.results_dir / "model_comparison.png",
            dpi=300,
            bbox_inches="tight",
        )
        print(f"  âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {self.results_dir / 'model_comparison.png'}")

        plt.close()

        # æ··æ·†çŸ©é˜µ
        fig, axes = plt.subplots(1, len(models), figsize=(15, 4))

        for idx, model_name in enumerate(models):
            cm = np.array(all_results[model_name]["confusion_matrix"])

            sns.heatmap(
                cm,
                annot=True,
                fmt="d",
                cmap="Blues",
                xticklabels=["Negative", "Positive"],
                yticklabels=["Negative", "Positive"],
                ax=axes[idx],
                cbar=False,
            )

            axes[idx].set_title(model_name, fontsize=11)
            axes[idx].set_xlabel("Predicted")
            axes[idx].set_ylabel("True")

        plt.tight_layout()
        plt.savefig(
            self.results_dir / "confusion_matrices.png",
            dpi=300,
            bbox_inches="tight",
        )
        print(f"  âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {self.results_dir / 'confusion_matrices.png'}")

        plt.close()

    def generate_report(self, all_results):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")

        comparison_data = []

        for model_name, results in all_results.items():
            comparison_data.append({
                "Model": model_name,
                "Accuracy": f"{results['accuracy']:.4f}",
                "Precision": f"{results['precision']:.4f}",
                "Recall": f"{results['recall']:.4f}",
                "F1 Score": f"{results['f1']:.4f}",
                "Macro F1": f"{results['f1_macro']:.4f}",
            })

        df = pd.DataFrame(comparison_data)

        report_file = self.results_dir / "evaluation_report.md"

        with open(report_file, "w", encoding="utf-8") as f:
            f.write("# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n")

            # æ‰‹åŠ¨ç”Ÿæˆ Markdown è¡¨æ ¼ï¼Œé¿å…ä¾èµ– tabulate
            headers = ["Model", "Accuracy", "Precision", "Recall", "F1 Score", "Macro F1"]
            # è¡¨å¤´
            f.write("| " + " | ".join(headers) + " |\n")
            # åˆ†éš”è¡Œ
            f.write("|" + "|".join([" --- " for _ in headers]) + "|\n")
            # æ¯ä¸€è¡Œæ•°æ®
            for _, row in df.iterrows():
                f.write(
                    "| "
                    + " | ".join(
                        str(row[h]) for h in headers
                    )
                    + " |\n"
                )

            f.write("\n\n")

            f.write("## è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n\n")
            for model_name, results in all_results.items():
                f.write(f"### {model_name}\n\n")
                f.write("```\n")
                f.write(results["classification_report"])
                f.write("\n```\n\n")

            f.write("## ç»“è®º\n\n")

            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆæŒ‰ weighted F1ï¼‰
            best_model = max(all_results.items(), key=lambda x: x[1]["f1"])
            f.write(f"- **æœ€ä½³æ¨¡å‹**: {best_model[0]}\n")
            f.write(f"- **F1åˆ†æ•°**: {best_model[1]['f1']:.4f}\n")
            f.write(f"- **å‡†ç¡®ç‡**: {best_model[1]['accuracy']:.4f}\n\n")

            if "BERT Baseline" in all_results and "BERT + SFT + DPO" in all_results:
                baseline_f1 = all_results["BERT Baseline"]["f1"]
                final_f1 = all_results["BERT + SFT + DPO"]["f1"]
                improvement = ((final_f1 - baseline_f1) / baseline_f1) * 100

                f.write(f"- **ç›¸å¯¹æ”¹è¿›**: {improvement:.2f}%\n")
                f.write(f"  - Baseline F1: {baseline_f1:.4f}\n")
                f.write(f"  - Final F1: {final_f1:.4f}\n")

        print(f"  âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        json_file = self.results_dir / "all_results.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)

        print(f"  âœ… JSONç»“æœå·²ä¿å­˜: {json_file}")

    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹ç»¼åˆè¯„ä¼°")
        print("=" * 60)

        test_data = self.load_test_data()

        all_results = {}
        all_predictions = {}

        for model_name, model_path in self.models.items():
            result = self.evaluate_model(model_name, model_path, test_data)
            if result is not None:
                results, predictions = result
                all_results[model_name] = results
                all_predictions[model_name] = predictions

        if not all_results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            return

        self.qualitative_analysis(test_data, all_predictions)
        self.plot_comparison(all_results)
        self.generate_report(all_results)

        print("\n" + "=" * 60)
        print("âœ… ç»¼åˆè¯„ä¼°å®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {self.results_dir}")


if __name__ == "__main__":
    evaluator = ModelEvaluator()
    evaluator.run_evaluation()

    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ: python scripts/6_demo_app.py")
