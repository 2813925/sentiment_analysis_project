#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å‡†å¤‡è„šæœ¬ - å®Œæ•´ä¿®å¤ç‰ˆ
ä¸‹è½½å¹¶å¤„ç†ChnSentiCorpæ•°æ®é›† + ç”Ÿæˆè‡ªæ„æ•°æ®
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from datasets import Dataset, DatasetDict

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
DPO_DATA_DIR = DATA_DIR / "dpo_pairs"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
DPO_DATA_DIR.mkdir(parents=True, exist_ok=True)


def download_dataset():
    """ä¸‹è½½ChnSentiCorpæ•°æ®é›†"""
    print("ğŸ“¥ ä¸‹è½½ChnSentiCorpæ•°æ®é›†...")

    dataset_path = RAW_DATA_DIR / "ChnSentiCorp" / "ChnSentiCorp_htl_all.csv"

    if dataset_path.exists():
        print("âœ… æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return dataset_path

    # åˆ›å»ºç›®å½•
    dataset_path.parent.mkdir(parents=True, exist_ok=True)

    # æç¤ºç”¨æˆ·æ‰‹åŠ¨ä¸‹è½½
    print("\nâš ï¸  éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†ï¼")
    print("è¯·è®¿é—®: https://github.com/SophonPlus/ChineseNlpCorpus")
    print("ä¸‹è½½: ChnSentiCorp_htl_all.csv")
    print(f"ä¿å­˜åˆ°: {dataset_path}")

    # å°è¯•è‡ªåŠ¨ä¸‹è½½
    try:
        import requests
        url = "https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv"

        print("ğŸŒ å°è¯•è‡ªåŠ¨ä¸‹è½½...")
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        with open(dataset_path, 'wb') as f:
            f.write(response.content)
        print("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ")

    except Exception as e:
        print(f"âš ï¸  è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {str(e)}")
        print("è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†")
        return None

    return dataset_path


def process_chnsenticorp(dataset_path):
    """å¤„ç†ChnSentiCorpæ•°æ®é›†"""
    print("\nğŸ“Š å¤„ç†ChnSentiCorpæ•°æ®é›†...")

    # è¯»å–æ•°æ®
    df = pd.read_csv(dataset_path)
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"åˆ—å: {df.columns.tolist()}")

    # æ•°æ®æ¸…æ´—
    df = df.dropna()  # åˆ é™¤ç¼ºå¤±å€¼
    df = df[df['review'].str.len() > 10]  # è¿‡æ»¤å¤ªçŸ­çš„è¯„è®º
    print(f"æ¸…æ´—åæ ·æœ¬æ•°: {len(df)}")

    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"æ­£é¢æ ·æœ¬: {(df['label'] == 1).sum()}")
    print(f"è´Ÿé¢æ ·æœ¬: {(df['label'] == 0).sum()}")

    # åˆ’åˆ†æ•°æ®é›†
    train_df, temp_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['label']
    )
    val_df, test_df = train_test_split(
        temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']
    )

    print(f"âœ… è®­ç»ƒé›†: {len(train_df)} | éªŒè¯é›†: {len(val_df)} | æµ‹è¯•é›†: {len(test_df)}")

    # é‡å‘½ååˆ—ä»¥åŒ¹é…transformersæ ¼å¼
    train_df = train_df.rename(columns={'review': 'text'})
    val_df = val_df.rename(columns={'review': 'text'})
    test_df = test_df.rename(columns={'review': 'text'})

    # è½¬æ¢ä¸ºdatasetsæ ¼å¼å¹¶ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜ä¸º Datasets æ ¼å¼...")
    train_dataset = Dataset.from_pandas(train_df[['text', 'label']].reset_index(drop=True))
    val_dataset = Dataset.from_pandas(val_df[['text', 'label']].reset_index(drop=True))
    test_dataset = Dataset.from_pandas(test_df[['text', 'label']].reset_index(drop=True))

    # ä¿å­˜ä¸º datasets æ ¼å¼ï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
    train_dataset.save_to_disk(str(PROCESSED_DATA_DIR / "train"))
    val_dataset.save_to_disk(str(PROCESSED_DATA_DIR / "validation"))
    test_dataset.save_to_disk(str(PROCESSED_DATA_DIR / "test"))

    print(f"âœ… Datasetsæ ¼å¼æ•°æ®å·²ä¿å­˜åˆ°:")
    print(f"   - {PROCESSED_DATA_DIR / 'train'}")
    print(f"   - {PROCESSED_DATA_DIR / 'validation'}")
    print(f"   - {PROCESSED_DATA_DIR / 'test'}")

    return train_df, val_df, test_df


def generate_custom_data():
    """ç”Ÿæˆè‡ªæ„æ ‡æ³¨æ•°æ®ï¼ˆåŒ…å«å¤æ‚æƒ…æ„Ÿåœºæ™¯ï¼‰"""
    print("\nğŸ¨ ç”Ÿæˆè‡ªæ„æ ‡æ³¨æ•°æ®ï¼ˆç”¨äºDPOè®­ç»ƒï¼‰...")

    custom_samples = []

    # 1. è®½åˆºç±»ï¼ˆ100æ¡ï¼‰
    sarcastic_templates = [
        ("è¿™æœåŠ¡çœŸæ˜¯{adj}ï¼Œè®©æˆ‘{feel}å¾—ä¸è¡Œ", 0),
        ("å“‡ï¼Œ{thing}çœŸæ˜¯{adj}å•Šï¼Œ{result}", 0),
        ("å—¯ï¼Œ{aspect}{adj}ï¼Œæˆ‘å¤ª{feel}äº†", 0),
        ("å‘µå‘µï¼Œ{aspect}ç¡®å®{adj}ï¼Œ{result}", 0),
        ("å¯çœŸæ˜¯{adj}çš„{thing}ï¼Œæˆ‘éƒ½{feel}äº†", 0),
    ]

    adj_negative = ["å¥½", "æ£’", "ä¼˜ç§€", "å®Œç¾", "è´´å¿ƒ", "å‘¨åˆ°", "ç»†è‡´"]
    feel_negative = ["å¤±æœ›", "ç”Ÿæ°”", "æ— è¯­", "éƒé—·", "æ„¤æ€’", "å¯’å¿ƒ"]
    things = ["ä½“éªŒ", "è´¨é‡", "æ€åº¦", "æ•ˆç‡", "æœåŠ¡"]
    aspects = ["æœåŠ¡", "äº§å“", "ç¯å¢ƒ", "é€Ÿåº¦", "æ€åº¦"]
    results = ["çœŸè®©äººå¤±æœ›", "å®Œå…¨ä¸è¡Œ", "æ°”æ­»æˆ‘äº†", "å¤ªç³Ÿç³•äº†", "æ— æ³•æ¥å—"]

    for _ in range(100):
        template, label = sarcastic_templates[np.random.randint(0, len(sarcastic_templates))]
        text = template.format(
            adj=np.random.choice(adj_negative),
            feel=np.random.choice(feel_negative),
            thing=np.random.choice(things),
            aspect=np.random.choice(aspects),
            result=np.random.choice(results)
        )
        custom_samples.append({"text": text, "label": label})

    # 2. éšå–»ç±»ï¼ˆ100æ¡ï¼‰
    metaphor_samples = [
                           ("ä½è¿›è¿™å®¶é…’åº—å°±åƒå›åˆ°äº†å…«åå¹´ä»£", 0),
                           ("æœåŠ¡å‘˜çš„æ€åº¦èƒ½å†»æ­»äºº", 0),
                           ("è¿™ä¸ªæˆ¿é—´ç®€ç›´æ˜¯æ¡‘æ‹¿æˆ¿", 0),
                           ("åºŠç¡¬å¾—åƒç¡åœ¨åœ°æ¿ä¸Š", 0),
                           ("éš”éŸ³æ•ˆæœç­‰äºé›¶", 0),
                           ("å«ç”Ÿé—´å°å¾—è½¬ä¸å¼€èº«", 0),
                           ("ç©ºè°ƒå£°éŸ³å¤§å¾—åƒæ‹–æ‹‰æœº", 0),
                           ("æ—©é¤éš¾åƒå¾—åƒçŒªé£Ÿ", 0),
                           ("ç½‘é€Ÿæ…¢å¾—åƒèœ—ç‰›", 0),
                           ("ä»·æ ¼è´µå¾—ç¦»è°±", 0),
                           ("æ—©é¤ä¸°å¯Œå¾—åƒæ»¡æ±‰å…¨å¸­", 1),
                           ("æˆ¿é—´å¹²å‡€å¾—åƒæ–°è£…ä¿®çš„", 1),
                           ("æœåŠ¡å‘˜çƒ­æƒ…å¾—åƒæ˜¥é£", 1),
                           ("ç¡çœ è´¨é‡å¥½å¾—åƒåœ¨å®¶é‡Œ", 1),
                           ("æ€§ä»·æ¯”é«˜å¾—ä»¤äººæƒŠå–œ", 1),
                           ("åœ°ç†ä½ç½®å¥½å¾—æ²¡è¯è¯´", 1),
                           ("è£…ä¿®è±ªåå¾—åƒäº”æ˜Ÿçº§", 1),
                           ("åºŠèˆ’æœå¾—åƒäº‘æœµ", 1),
                           ("æœåŠ¡å‘¨åˆ°å¾—æ— å¯æŒ‘å‰”", 1),
                           ("ç¯å¢ƒä¼˜é›…å¾—åƒåº¦å‡æ‘", 1),
                       ] * 5

    for text, label in metaphor_samples:
        custom_samples.append({"text": text, "label": label})

    # 3. åŒé‡å¦å®šï¼ˆ100æ¡ï¼‰
    double_negative = [
                          ("ä¸å¾—ä¸è¯´ï¼Œè¿™å®¶é…’åº—ä¸å·®", 1),
                          ("è¯´å®è¯ï¼Œæ²¡æœ‰ä»€ä¹ˆä¸æ»¡æ„çš„", 1),
                          ("ä¸èƒ½è¯´ä¸å¥½ï¼Œä½†ä¹Ÿä¸æ˜¯ç‰¹åˆ«å¥½", 0),
                          ("å¹¶éä¸èƒ½æ¥å—ï¼Œä½†ç¡®å®ä¸å¤ªæ»¡æ„", 0),
                          ("æ²¡ä»€ä¹ˆä¸å¥½çš„ï¼Œå°±æ˜¯ä»·æ ¼ä¸ä¾¿å®œ", 0),
                          ("ä¸æ˜¯ä¸æ¨èï¼Œåªæ˜¯æ€§ä»·æ¯”ä¸é«˜", 0),
                          ("ä¸èƒ½è¯´æœåŠ¡ä¸å¥½ï¼Œä½†ä¹Ÿè°ˆä¸ä¸Šçƒ­æƒ…", 0),
                          ("ä¸æ˜¯å®Œå…¨ä¸èƒ½ä½ï¼Œä½†ä¸‹æ¬¡ä¸ä¼šå†æ¥", 0),
                          ("æ²¡æœ‰ä¸å¹²å‡€ï¼Œä½†ä¹Ÿç®—ä¸ä¸Šæ•´æ´", 0),
                          ("ä¸å¾—ä¸æ‰¿è®¤ï¼Œç¡®å®ä¸é”™", 1),
                          ("ä¸èƒ½å¦è®¤ï¼Œè¿™é‡Œå¾ˆæ£’", 1),
                          ("è¯´ä¸ä¸Šä¸å–œæ¬¢ï¼Œè¿˜æŒºæ»¡æ„çš„", 1),
                          ("æ²¡æœ‰ä¸æ¨èçš„ç†ç”±", 1),
                          ("ä¸æ˜¯æ²¡æœ‰ç¼ºç‚¹ï¼Œä½†ç‘•ä¸æ©ç‘œ", 1),
                      ] * 7 + [
                          ("å¹¶éå®Œç¾æ— ç¼ºï¼Œä½†æ•´ä½“ä¸é”™", 1),
                          ("ä¸æ˜¯è¯´æ²¡æœ‰é—®é¢˜ï¼Œä½†å¯ä»¥æ¥å—", 1),
                      ] * 3

    for text, label in double_negative[:100]:
        custom_samples.append({"text": text, "label": label})

    # 4. å¯¹æ¯”è½¬æŠ˜ï¼ˆ100æ¡ï¼‰
    contrast_templates = [
        ("è™½ç„¶{positive}ï¼Œä½†æ˜¯{negative}", 0),
        ("{negative}ï¼Œä¸è¿‡{positive}", 1),
        ("æœ¬æ¥{negative}ï¼Œç»“æœ{positive}", 1),
        ("{positive}ï¼Œå¯æƒœ{negative}", 0),
        ("é™¤äº†{negative}ï¼Œå…¶ä»–éƒ½{positive}", 1),
        ("æ•´ä½“{positive}ï¼Œå°±æ˜¯{negative}", 0),
    ]

    positives = ["æœåŠ¡å¾ˆå¥½", "ç¯å¢ƒä¸é”™", "ä½ç½®ä¾¿åˆ©", "æˆ¿é—´å¹²å‡€", "è®¾æ–½é½å…¨", "æ€§ä»·æ¯”é«˜"]
    negatives = ["ä»·æ ¼å¤ªè´µ", "éš”éŸ³å¾ˆå·®", "è®¾æ–½é™ˆæ—§", "æ—©é¤éš¾åƒ", "æˆ¿é—´å¤ªå°", "ç½‘ç»œå¾ˆæ…¢"]

    for _ in range(100):
        template, label = contrast_templates[np.random.randint(0, len(contrast_templates))]
        text = template.format(
            positive=np.random.choice(positives),
            negative=np.random.choice(negatives)
        )
        custom_samples.append({"text": text, "label": label})

    # 5. ç»†ç²’åº¦æƒ…æ„Ÿï¼ˆ100æ¡ï¼‰
    fine_grained = [
                       ("æ€»ä½“è¿˜å¯ä»¥ï¼Œå°±æ˜¯æœ‰äº›å°ç‘•ç–µ", 1),
                       ("åŸºæœ¬æ»¡æ„ï¼Œæ€§ä»·æ¯”å°šå¯", 1),
                       ("ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„", 0),
                       ("å‹‰å¼ºèƒ½ä½ï¼Œå‡‘åˆä¸€æ™š", 0),
                       ("è¿˜è¡Œå§ï¼Œä¸‹æ¬¡å¯èƒ½ä¸ä¼šå†æ¥", 0),
                       ("ä¸­è§„ä¸­çŸ©ï¼Œç¬¦åˆé¢„æœŸ", 1),
                       ("å°šå¯æ¥å—ï¼Œä½†è°ˆä¸ä¸ŠæƒŠå–œ", 0),
                       ("è¶…å‡ºé¢„æœŸï¼Œç›¸å½“ä¸é”™", 1),
                       ("å·®å¼ºäººæ„ï¼Œå‹‰å¼ºåŠæ ¼", 0),
                       ("ç‰©æœ‰æ‰€å€¼ï¼Œæ¨èå…¥ä½", 1),
                       ("æ€§ä»·æ¯”ä¸€èˆ¬ï¼Œä¸å¤ªæ¨è", 0),
                       ("æ•´ä½“æ»¡æ„ï¼Œä¼šå†æ¬¡å…‰é¡¾", 1),
                       ("ä½“éªŒå¹³å¹³ï¼Œæ²¡æœ‰äº®ç‚¹", 0),
                       ("ç›¸å½“æ»¡æ„ï¼Œå€¼å¾—æ¨è", 1),
                       ("ç•¥æ˜¾å¤±æœ›ï¼ŒæœŸæœ›è¿‡é«˜", 0),
                   ] * 7 + [
                       ("æ•´ä½“ä¸é”™ï¼Œæœ‰å¾…æ”¹è¿›", 1),
                   ] * 5

    for text, label in fine_grained[:100]:
        custom_samples.append({"text": text, "label": label})

    # ä¿å­˜
    custom_data_path = RAW_DATA_DIR / "custom_data.json"
    with open(custom_data_path, 'w', encoding='utf-8') as f:
        json.dump(custom_samples, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç”Ÿæˆäº† {len(custom_samples)} æ¡è‡ªæ„æ•°æ®: {custom_data_path}")

    return custom_samples


def create_sft_data(train_df, val_df, test_df):
    """åˆ›å»ºSFTæŒ‡ä»¤æ ¼å¼æ•°æ®"""
    instruction = "è¯·åˆ¤æ–­ä»¥ä¸‹é…’åº—è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼Œå›ç­”'æ­£é¢'æˆ–'è´Ÿé¢'ã€‚"

    def convert_to_sft_format(df, output_file):
        print(f"ğŸ¯ åˆ›å»ºSFTæŒ‡ä»¤æ ¼å¼æ•°æ®: {output_file}")
        sft_data = []

        for _, row in tqdm(df.iterrows(), total=len(df), desc="è½¬æ¢æ ¼å¼"):
            text = row['text']
            label = "æ­£é¢" if row['label'] == 1 else "è´Ÿé¢"

            sample = {
                "instruction": instruction,
                "input": text,
                "output": label
            }
            sft_data.append(sample)

        # ä¿å­˜
        output_path = PROCESSED_DATA_DIR / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(sft_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç”Ÿæˆ {len(sft_data)} æ¡SFTæ•°æ®")
        return sft_data

    # è½¬æ¢ä¸‰ä¸ªæ•°æ®é›†
    train_sft = convert_to_sft_format(train_df, "sft_train.json")
    val_sft = convert_to_sft_format(val_df, "sft_dev.json")
    test_sft = convert_to_sft_format(test_df, "sft_test.json")

    return train_sft, val_sft, test_sft


def create_dpo_pairs(custom_samples):
    """åˆ›å»ºDPOåå¥½å¯¹æ•°æ®"""
    print("\nğŸ”„ åˆ›å»ºDPOåå¥½å¯¹...")

    dpo_pairs = []

    # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œåˆ›å»ºchosenå’Œrejectedå“åº”
    for i in range(min(200, len(custom_samples))):
        sample = custom_samples[i]

        correct_label = "æ­£é¢" if sample['label'] == 1 else "è´Ÿé¢"
        wrong_label = "è´Ÿé¢" if sample['label'] == 1 else "æ­£é¢"

        dpo_pair = {
            "prompt": f"è¯·åˆ¤æ–­ä»¥ä¸‹é…’åº—è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼š{sample['text']}",
            "chosen": correct_label,
            "rejected": wrong_label
        }
        dpo_pairs.append(dpo_pair)

    # ä¿å­˜
    dpo_train_path = DPO_DATA_DIR / "dpo_train.json"
    with open(dpo_train_path, 'w', encoding='utf-8') as f:
        json.dump(dpo_pairs, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç”Ÿæˆ {len(dpo_pairs)} å¯¹DPOè®­ç»ƒæ•°æ®: {dpo_train_path}")

    return dpo_pairs


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹")
    print("=" * 60)

    # 1. ä¸‹è½½æ•°æ®é›†
    dataset_path = download_dataset()
    if dataset_path is None or not dataset_path.exists():
        print("\nâŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½ï¼")
        print(f"ä¸‹è½½é“¾æ¥: https://github.com/SophonPlus/ChineseNlpCorpus")
        print(f"ä¿å­˜ä½ç½®: {RAW_DATA_DIR / 'ChnSentiCorp' / 'ChnSentiCorp_htl_all.csv'}")
        return

    # 2. å¤„ç†ChnSentiCorpæ•°æ®
    train_df, val_df, test_df = process_chnsenticorp(dataset_path)

    # 3. ç”Ÿæˆè‡ªæ„æ•°æ®
    custom_samples = generate_custom_data()

    # 4. åˆ›å»ºSFTæ•°æ®
    train_sft, val_sft, test_sft = create_sft_data(train_df, val_df, test_df)

    # 5. åˆ›å»ºDPOæ•°æ®
    dpo_pairs = create_dpo_pairs(custom_samples)

    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ æ•°æ®ä¿å­˜ä½ç½®:")
    print(f"  - Datasetsæ ¼å¼: {PROCESSED_DATA_DIR}")
    print(f"  - SFTæ•°æ®: {PROCESSED_DATA_DIR}/sft_*.json")
    print(f"  - DPOæ•°æ®: {DPO_DATA_DIR}/dpo_train.json")

    # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ” éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:")
    train_exists = (PROCESSED_DATA_DIR / 'train').exists()
    val_exists = (PROCESSED_DATA_DIR / 'validation').exists()
    test_exists = (PROCESSED_DATA_DIR / 'test').exists()

    print(f"  âœ… train dataset: {train_exists}")
    print(f"  âœ… validation dataset: {val_exists}")
    print(f"  âœ… test dataset: {test_exists}")

    if not (train_exists and val_exists and test_exists):
        print("\nâš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†æ•°æ®é›†æœªæˆåŠŸç”Ÿæˆï¼")
    else:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")


if __name__ == "__main__":
    main()