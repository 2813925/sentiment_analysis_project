"""
Gradio Demoåº”ç”¨ - Step 6
äº¤äº’å¼æƒ…æ„Ÿåˆ†ææ¼”ç¤º
"""

import os
import torch
import gradio as gr
from pathlib import Path
from transformers import BertTokenizer, BertForSequenceClassification

# åªä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å…å» HuggingFace è”ç½‘
os.environ["TRANSFORMERS_OFFLINE"] = "1"


class SentimentAnalysisDemo:
    def __init__(self, base_dir: str = "./"):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé˜²æ­¢å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´è·¯å¾„é”™è¯¯
        self.base_dir = Path(base_dir).resolve()
        self.models_dir = self.base_dir / "models"

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½æ‰€æœ‰å¯ç”¨æ¨¡å‹
        self.models = {}
        self.tokenizers = {}

        self.load_models()

        self.max_length = 256

    def load_models(self):
        """åŠ è½½æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")

        # Baseline ä½¿ç”¨ final ç›®å½•ï¼Œå…¶ä½™ä½¿ç”¨å„è‡ªç›®å½•
        model_configs = {
            "BERT Baseline": self.models_dir / "bert_baseline" / "final",
            "BERT + SFT": self.models_dir / "bert_sft",
            "BERT + SFT + DPO": self.models_dir / "bert_dpo",
        }

        for name, path in model_configs.items():
            if path.exists():
                try:
                    tokenizer = BertTokenizer.from_pretrained(
                        str(path),
                        local_files_only=True,
                    )
                    model = BertForSequenceClassification.from_pretrained(
                        str(path),
                        local_files_only=True,
                    )
                    model.to(self.device)
                    model.eval()

                    self.models[name] = model
                    self.tokenizers[name] = tokenizer

                    print(f"  âœ… {name}")
                except Exception as e:
                    print(f"  âŒ {name}: {e}")
            else:
                print(f"  âš ï¸  {name}: æ¨¡å‹ä¸å­˜åœ¨ -> {path}")

        if not self.models:
            print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬:")
            print("  python scripts/2_baseline_training.py")
            print("  python scripts/3_sft_training.py")
            print("  python scripts/4_dpo_training.py")

    def predict(self, text, model_name):
        """é¢„æµ‹å•ä¸ªæ–‡æœ¬"""
        if model_name not in self.models:
            return "âŒ æ¨¡å‹æœªåŠ è½½", None

        if not text.strip():
            return "è¯·è¾“å…¥è¯„è®ºæ–‡æœ¬", None

        # è·å–æ¨¡å‹å’Œtokenizer
        model = self.models[model_name]
        tokenizer = self.tokenizers[model_name]

        # æ„é€ è¾“å…¥ï¼ˆSFT/DPO ç”¨æŒ‡ä»¤æ ¼å¼ï¼ŒBaseline ç›´æ¥ç”¨åŸå§‹æ–‡æœ¬ï¼‰
        if model_name != "BERT Baseline":
            instruction = "ä»»åŠ¡ï¼šåˆ¤æ–­ä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚"
            input_text = f"{instruction}\nè¯„è®ºï¼š{text}\nç­”æ¡ˆï¼š"
        else:
            input_text = text

        # Tokenize
        inputs = tokenizer(
            input_text,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt",
        ).to(self.device)

        # é¢„æµ‹
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
            pred = torch.argmax(logits, dim=1).item()

        # ç»“æœ
        sentiment = "ğŸ˜Š æ­£é¢" if pred == 1 else "ğŸ˜ è´Ÿé¢"
        confidence = probs[pred] * 100

        # è¯¦ç»†ä¿¡æ¯
        result_text = f"""
### åˆ†æç»“æœ

**æƒ…æ„Ÿå€¾å‘**: {sentiment}  
**ç½®ä¿¡åº¦**: {confidence:.2f}%

**è¯¦ç»†æ¦‚ç‡**:
- è´Ÿé¢: {probs[0] * 100:.2f}%
- æ­£é¢: {probs[1] * 100:.2f}%
"""

        # è¿”å›æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºå›¾è¡¨ï¼‰
        prob_dict = {
            "è´Ÿé¢": float(probs[0]),
            "æ­£é¢": float(probs[1]),
        }

        return result_text, prob_dict

    def predict_all_models(self, text):
        """ä½¿ç”¨æ‰€æœ‰æ¨¡å‹é¢„æµ‹"""
        if not text.strip():
            return "è¯·è¾“å…¥è¯„è®ºæ–‡æœ¬"

        results = "## æ‰€æœ‰æ¨¡å‹å¯¹æ¯”ç»“æœ\n\n"

        for model_name in self.models.keys():
            result, _ = self.predict(text, model_name)
            results += f"### {model_name}\n{result}\n---\n\n"

        return results

    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""

        # ç¤ºä¾‹æ–‡æœ¬
        examples = [
            ["è¿™å®¶åº—çš„æœåŠ¡çœŸæ˜¯å¤ªè´´å¿ƒäº†ï¼Œæ¯ä¸ªç»†èŠ‚éƒ½è€ƒè™‘åˆ°äº†ï¼"],
            ["äº§å“è´¨é‡è¶…å‡ºé¢„æœŸï¼Œç‰©è¶…æ‰€å€¼ï¼Œå¼ºçƒˆæ¨èï¼"],
            ["è¿™ä»€ä¹ˆç ´ç©æ„å„¿ï¼Œç”¨äº†ä¸¤å¤©å°±åäº†ï¼Œå¤ªå¤±æœ›äº†ã€‚"],
            ["å®¢æœæ€åº¦æå·®ï¼Œé—®é¢˜è¿Ÿè¿Ÿå¾—ä¸åˆ°è§£å†³ã€‚"],
            ["çœŸæ˜¯'ç‰©ç¾ä»·å»‰'å•Šï¼Œä¸€åˆ†é’±ä¸€åˆ†è´§éƒ½ä¸å¦‚ã€‚"],
            ["è¿˜è¡Œå§ï¼Œå‡‘åˆèƒ½ç”¨ã€‚"],
        ]

        # ä¸»é¢˜CSS
        custom_css = """
        .gradio-container {
            font-family: 'Arial', sans-serif;
        }
        .output-markdown {
            font-size: 16px;
        }
        """

        with gr.Blocks(
            title="ä¸­æ–‡è¯„è®ºæƒ…æ„Ÿåˆ†æ",
            css=custom_css,
            theme=gr.themes.Soft()
        ) as demo:

            gr.Markdown(
                """
            # ğŸ¯ ä¸­æ–‡è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ
            
            åŸºäº BERT çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œæ”¯æŒ **SFT** å’Œ **DPO** å¾®è°ƒ
            
            ---
            """
            )

            # å•æ¨¡å‹é¢„æµ‹
            with gr.Tab("å•æ¨¡å‹é¢„æµ‹"):
                with gr.Row():
                    with gr.Column(scale=2):
                        input_text = gr.Textbox(
                            label="è¾“å…¥è¯„è®º",
                            placeholder="è¯·è¾“å…¥ä¸­æ–‡è¯„è®ºæ–‡æœ¬...",
                            lines=3,
                        )

                        model_choice = gr.Dropdown(
                            choices=list(self.models.keys()),
                            value=list(self.models.keys())[0]
                            if self.models
                            else None,
                            label="é€‰æ‹©æ¨¡å‹",
                        )

                        predict_btn = gr.Button("ğŸ” åˆ†ææƒ…æ„Ÿ", variant="primary")

                    with gr.Column(scale=3):
                        output_text = gr.Markdown(label="åˆ†æç»“æœ")
                        # ç”¨ Label å±•ç¤ºæ¦‚ç‡
                        output_plot = gr.Label(
                            label="æ¦‚ç‡åˆ†å¸ƒ", num_top_classes=2
                        )

                gr.Examples(
                    examples=examples,
                    inputs=input_text,
                    label="ç¤ºä¾‹è¯„è®º",
                )

                predict_btn.click(
                    fn=self.predict,
                    inputs=[input_text, model_choice],
                    outputs=[output_text, output_plot],
                )

            # æ¨¡å‹å¯¹æ¯”
            with gr.Tab("æ¨¡å‹å¯¹æ¯”"):
                with gr.Row():
                    with gr.Column(scale=1):
                        compare_input = gr.Textbox(
                            label="è¾“å…¥è¯„è®º",
                            placeholder="è¯·è¾“å…¥ä¸­æ–‡è¯„è®ºæ–‡æœ¬...",
                            lines=4,
                        )
                        compare_btn = gr.Button(
                            "ğŸ”„ å¯¹æ¯”æ‰€æœ‰æ¨¡å‹", variant="primary"
                        )

                    with gr.Column(scale=2):
                        compare_output = gr.Markdown(label="å¯¹æ¯”ç»“æœ")

                gr.Examples(
                    examples=examples,
                    inputs=compare_input,
                    label="ç¤ºä¾‹è¯„è®º",
                )

                compare_btn.click(
                    fn=self.predict_all_models,
                    inputs=compare_input,
                    outputs=compare_output,
                )

            # é¡¹ç›®è¯´æ˜
            with gr.Tab("é¡¹ç›®è¯´æ˜"):
                gr.Markdown(
                    """
                ## ğŸ“– é¡¹ç›®ä»‹ç»
                
                æœ¬é¡¹ç›®å®ç°äº†åŸºäº BERT çš„ä¸­æ–‡è¯„è®ºæƒ…æ„Ÿåˆ†æï¼Œé‡‡ç”¨ä»¥ä¸‹æŠ€æœ¯è·¯çº¿ï¼š
                
                ### ğŸ”§ æŠ€æœ¯æ ˆ
                
                1. **Baseline**: BERT-base-chinese
                2. **SFT (Supervised Fine-Tuning)**: æŒ‡ä»¤å¼å¾®è°ƒ
                3. **DPO (Direct Preference Optimization)**: åå¥½ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
                
                ### ğŸ“Š æ•°æ®é›†
                
                - **ChnSentiCorp**: ä¸­æ–‡æƒ…æ„Ÿåˆ†æè¯­æ–™
                - **åå¥½å¯¹æ•°æ®**: çº¦ 200 æ¡äººå·¥æ ‡æ³¨çš„åå¥½å¯¹ï¼ˆprompt + chosen + rejectedï¼‰
                
                ### ğŸ¯ åœ¨æµ‹è¯•é›†ä¸Šçš„å®é™…æ€§èƒ½ï¼ˆæœ¬é¡¹ç›®å®æµ‹ç»“æœï¼‰
                
                | æ¨¡å‹ | Accuracy | F1 Score | Macro F1 |
                |------|----------|----------|----------|
                | BERT Baseline | 0.8943 | 0.8966 | 0.8832 |
                | BERT + SFT | 0.9188 | 0.9188 | 0.9057 |
                | BERT + SFT + DPO | 0.9137 | 0.9150 | 0.9032 |
                
                ### ğŸŒŸ é¡¹ç›®äº®ç‚¹
                
                - âœ… å®Œæ•´çš„è®­ç»ƒ pipelineï¼ˆBaseline â†’ SFT â†’ DPOï¼‰
                - âœ… åˆ©ç”¨åå¥½å¯¹æ•°æ®è¿›è¡Œå¯¹é½å¾®è°ƒ
                - âœ… è¯¦ç»†çš„æ¨¡å‹å¯¹æ¯”å’Œå¯è§†åŒ–ï¼ˆæ··æ·†çŸ©é˜µã€æŒ‡æ ‡å¯¹æ¯”ï¼‰
                - âœ… äº¤äº’å¼ Gradio Demo åº”ç”¨
                
                ---
                
                æœ¬ Demo ç”¨äºè¯¾ç¨‹/é¡¹ç›®å±•ç¤ºï¼Œæ–¹ä¾¿ç›´è§‚ä½“éªŒä¸åŒè®­ç»ƒé˜¶æ®µæ¨¡å‹çš„æ•ˆæœå·®å¼‚ã€‚
                """
                )

        return demo

    def launch(self, share: bool = False):
        """å¯åŠ¨åº”ç”¨"""
        if not self.models:
            print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼")
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹åå†å¯åŠ¨Demoã€‚")
            return

        demo = self.create_interface()

        print("\n" + "=" * 60)
        print("ğŸš€ å¯åŠ¨ Gradio Demo")
        print("=" * 60)
        print(f"å·²åŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹")
        print("\nè®¿é—®åœ°å€å°†åœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºï¼ˆæ³¨æ„ä½¿ç”¨ 127.0.0.1 æˆ– æœåŠ¡å™¨IP è®¿é—®ï¼Œä¸è¦ç”¨ 0.0.0.0ï¼‰")

        demo.launch(
            share=share,
            server_name="0.0.0.0",  # ç›‘å¬æ‰€æœ‰ç½‘å¡
            server_port=7860,
            show_error=True,
        )


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--share", action="store_true", help="åˆ›å»ºå…¬å¼€åˆ†äº«é“¾æ¥"
    )
    args = parser.parse_args()

    demo_app = SentimentAnalysisDemo()
    demo_app.launch(share=args.share)
