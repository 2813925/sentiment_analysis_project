"""
SFT (Supervised Fine-Tuning) è®­ç»ƒè„šæœ¬ - Step 3
ä½¿ç”¨æŒ‡ä»¤æ ¼å¼å¯¹BERTè¿›è¡Œå¾®è°ƒ
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    DataCollatorWithPadding
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt


class SFTTrainer:
    def __init__(self, base_dir: str = "./"):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé˜²æ­¢å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´æ‰¾ä¸åˆ°æ–‡ä»¶
        self.base_dir = Path(base_dir).resolve()
        self.data_dir = self.base_dir / "data" / "processed"
        self.model_dir = self.base_dir / "models" / "bert_sft"
        self.results_dir = self.base_dir / "results" / "sft"

        # åˆ›å»ºç›®å½•
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {self.base_dir}")

        # æ¨¡å‹å‚æ•°ï¼šæ”¹ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„
        # ä½¿ç”¨åŸå§‹é¢„è®­ç»ƒ BERT ä½œä¸º SFT èµ·ç‚¹
        self.model_name = str(self.base_dir / "models" / "bert-base-chinese")

        # å¦‚æœä½ æƒ³åŸºäº baseline ç»§ç»­è®­ç»ƒï¼Œå¯ä»¥æ”¹æˆè¿™ä¸€è¡Œï¼š
        # self.model_name = str(self.base_dir / "models" / "bert_baseline" / "final")

        self.max_length = 256  # SFTéœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡
        self.num_labels = 2

        # å¼ºåˆ¶åªç”¨æœ¬åœ°ç¼“å­˜ï¼Œç¦æ­¢è”ç½‘
        os.environ["TRANSFORMERS_OFFLINE"] = "1"

    def load_sft_data(self):
        """åŠ è½½SFTæ ¼å¼æ•°æ®"""
        print("\nğŸ“Š åŠ è½½SFTæ•°æ®...")

        with open(self.data_dir / "sft_train.json", 'r', encoding='utf-8') as f:
            train_data = json.load(f)

        with open(self.data_dir / "sft_dev.json", 'r', encoding='utf-8') as f:
            dev_data = json.load(f)

        with open(self.data_dir / "sft_test.json", 'r', encoding='utf-8') as f:
            test_data = json.load(f)

        print(f"è®­ç»ƒé›†: {len(train_data)} | éªŒè¯é›†: {len(dev_data)} | æµ‹è¯•é›†: {len(test_data)}")

        return train_data, dev_data, test_data

    def format_instruction(self, item):
        """æ ¼å¼åŒ–æŒ‡ä»¤å¼è¾“å…¥"""
        # item ç»“æ„ç¤ºä¾‹ï¼š
        # {
        #   "instruction": "è¯·åˆ¤æ–­ä»¥ä¸‹é…’åº—è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼Œå›ç­”'æ­£é¢'æˆ–'è´Ÿé¢'ã€‚",
        #   "input": "æˆ¿é—´å¾ˆå¹²å‡€ï¼Œå°±æ˜¯æœ‰ç‚¹å°ã€‚",
        #   "output": "æ­£é¢"
        # }
        full_text = f"{item['instruction']}\n{item['input']}\nç­”æ¡ˆï¼š"
        return full_text

    def label_to_id(self, label_text: str) -> int:
        """å°† 'æ­£é¢' / 'è´Ÿé¢' è½¬æˆ 1 / 0"""
        # ä¿é™©èµ·è§ï¼Œåšä¸€ä¸‹ strip
        label_text = str(label_text).strip()
        if label_text == "æ­£é¢":
            return 1
        elif label_text == "è´Ÿé¢":
            return 0
        # å¦‚æœä¸ç¬¦åˆé¢„æœŸï¼Œæ‰“å°ä¸€ä¸‹ï¼Œé»˜è®¤å½“ä½œè´Ÿé¢
        print(f"âš ï¸ æ„å¤–æ ‡ç­¾å€¼: {label_text}ï¼Œé»˜è®¤æ˜ å°„ä¸º 0ï¼ˆè´Ÿé¢ï¼‰")
        return 0

    def prepare_dataset(self, data, tokenizer):
        """å‡†å¤‡æ•°æ®é›†"""

        texts = []
        labels = []

        for item in data:
            # æ„é€ æŒ‡ä»¤å¼æ–‡æœ¬
            text = self.format_instruction(item)
            texts.append(text)

            # ä» output å­—æ®µè¯»å–æ ‡ç­¾ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
            label_id = self.label_to_id(item["output"])
            labels.append(label_id)

        # Tokenize
        encodings = tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        # åˆ›å»ºDataset
        dataset_dict = {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': torch.tensor(labels)
        }

        dataset = Dataset.from_dict(dataset_dict)

        return dataset

    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)

        acc = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='weighted')
        f1_macro = f1_score(labels, predictions, average='macro')

        return {
            'accuracy': acc,
            'f1': f1,
            'f1_macro': f1_macro
        }

    def train(self):
        """è®­ç»ƒSFTæ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹SFTè®­ç»ƒ")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        train_data, dev_data, test_data = self.load_sft_data()

        # åŠ è½½tokenizerå’Œæ¨¡å‹ï¼ˆåªç”¨æœ¬åœ°æ–‡ä»¶ï¼‰
        print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_name}")
        tokenizer = BertTokenizer.from_pretrained(
            self.model_name,
            local_files_only=True,  # å…³é”®ï¼šåªç”¨æœ¬åœ°
        )
        model = BertForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=self.num_labels,
            local_files_only=True,  # å…³é”®ï¼šåªç”¨æœ¬åœ°
        )

        # å‡†å¤‡æ•°æ®é›†
        print("\nğŸ”§ å‡†å¤‡SFTæ•°æ®é›†...")
        train_dataset = self.prepare_dataset(train_data, tokenizer)
        eval_dataset = self.prepare_dataset(dev_data, tokenizer)
        test_dataset = self.prepare_dataset(test_data, tokenizer)

        print("è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
        print(f"  è¾“å…¥: {self.format_instruction(train_data[0])[:100]}...")
        print(f"  æ ‡ç­¾(output): {train_data[0]['output']}")

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.model_dir),
            num_train_epochs=4,  # SFTå¯ä»¥å¤šè®­ç»ƒå‡ è½®
            per_device_train_batch_size=16,  # åºåˆ—æ›´é•¿ï¼Œbatch sizeå°ä¸€äº›
            per_device_eval_batch_size=16,
            learning_rate=3e-5,  # ç¨é«˜çš„å­¦ä¹ ç‡
            weight_decay=0.01,
            warmup_ratio=0.1,
            logging_dir=str(self.results_dir / "logs"),
            logging_steps=100,
            eval_strategy="steps",
            eval_steps=300,
            save_strategy="steps",
            save_steps=300,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            report_to="tensorboard",
            fp16=torch.cuda.is_available(),
        )

        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            compute_metrics=self.compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )

        # å¼€å§‹è®­ç»ƒ
        print("\nğŸ‹ï¸ å¼€å§‹SFTè®­ç»ƒ...")
        train_result = trainer.train()

        # ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜SFTæ¨¡å‹åˆ°: {self.model_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(self.model_dir)

        # è¯„ä¼°
        print("\nğŸ“Š è¯„ä¼°SFTæ¨¡å‹...")

        # éªŒè¯é›†è¯„ä¼°
        eval_results = trainer.evaluate(eval_dataset)
        print("\néªŒè¯é›†ç»“æœ:")
        for key, value in eval_results.items():
            try:
                print(f"  {key}: {value:.4f}")
            except TypeError:
                print(f"  {key}: {value}")

        # æµ‹è¯•é›†è¯„ä¼°
        test_results = trainer.evaluate(test_dataset)
        print("\næµ‹è¯•é›†ç»“æœ:")
        for key, value in test_results.items():
            try:
                print(f"  {key}: {value:.4f}")
            except TypeError:
                print(f"  {key}: {value}")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        predictions = trainer.predict(test_dataset)
        pred_labels = np.argmax(predictions.predictions, axis=1)
        true_labels = [self.label_to_id(item["output"]) for item in test_data]

        report = classification_report(
            true_labels,
            pred_labels,
            target_names=['è´Ÿé¢', 'æ­£é¢'],
            digits=4
        )
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(report)

        # ä¿å­˜ç»“æœ
        results = {
            'model_name': 'BERT + SFT',
            'train_samples': len(train_data),
            'eval_samples': len(dev_data),
            'test_samples': len(test_data),
            'eval_results': eval_results,
            'test_results': test_results,
            'classification_report': report,
            'training_time': train_result.metrics.get('train_runtime', None),
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(self.results_dir / "results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_dir / 'results.json'}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(trainer)

        # æµ‹è¯•å‡ ä¸ªä¾‹å­
        self.test_examples(model, tokenizer, test_data[:5])

        return trainer, results

    def test_examples(self, model, tokenizer, examples):
        """æµ‹è¯•ä¸€äº›ä¾‹å­"""
        print("\nğŸ§ª æµ‹è¯•ç¤ºä¾‹:")

        model.eval()
        model.to(self.device)

        for i, item in enumerate(examples):
            text = self.format_instruction(item)

            inputs = tokenizer(
                text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=self.max_length
            ).to(self.device)

            with torch.no_grad():
                outputs = model(**inputs)
                pred = torch.argmax(outputs.logits, dim=1).item()

            pred_label = 'æ­£é¢' if pred == 1 else 'è´Ÿé¢'
            true_label = item["output"]

            # SFT æ•°æ®é‡Œæ²¡æœ‰ review å­—æ®µï¼Œç”¨ input å½“ä½œåŸå§‹è¯„è®º
            review_text = item.get("input", "")[:50]

            print(f"\nç¤ºä¾‹ {i + 1}:")
            print(f"  è¯„è®º: {review_text}...")
            print(f"  çœŸå®æ ‡ç­¾(output): {true_label}")
            print(f"  é¢„æµ‹æ ‡ç­¾: {pred_label}")
            print("  âœ“" if pred_label == true_label else "  âœ—")

    def plot_training_curves(self, trainer):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")

        log_history = trainer.state.log_history

        # æå–è®­ç»ƒæŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡
        train_loss = [log['loss'] for log in log_history if 'loss' in log]
        eval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]
        eval_f1 = [log['eval_f1'] for log in log_history if 'eval_f1' in log]

        steps_train = [log['step'] for log in log_history if 'loss' in log]
        steps_eval = [log['step'] for log in log_history if 'eval_loss' in log]

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Lossæ›²çº¿
        axes[0].plot(steps_train, train_loss, label='Train Loss', marker='o', alpha=0.7)
        axes[0].plot(steps_eval, eval_loss, label='Eval Loss', marker='s', alpha=0.7)
        axes[0].set_xlabel('Steps')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('SFT Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # F1æ›²çº¿
        axes[1].plot(steps_eval, eval_f1, label='Eval F1', marker='s', alpha=0.7)
        axes[1].set_xlabel('Steps')
        axes[1].set_ylabel('F1 Score')
        axes[1].set_title('SFT Validation F1 Score')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.results_dir / 'sft_training_curves.png', dpi=300, bbox_inches='tight')
        print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.results_dir / 'sft_training_curves.png'}")

        plt.close()


if __name__ == "__main__":
    trainer = SFTTrainer()
    trainer.train()

    print("\n" + "=" * 60)
    print("âœ… SFTè®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ: python scripts/4_dpo_training.py")
